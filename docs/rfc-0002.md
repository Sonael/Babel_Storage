# RFC 0002: Checksums por Chunk (BSP v2)

**Status**: Implementado
**Versão**: BSP v2
**Data**: 2026-02-16
**Autores**: Projeto BabelStorage

## Resumo

Este documento especifica o mecanismo de integridade por chunk introduzido no BSP v2. Enquanto o BSP v1 fornece verificação SHA-256 no nível do arquivo, o BSP v2 adiciona checksums SHA-256 individuais para cada chunk. Isso permite a detecção precoce de corrupção sem baixar o arquivo inteiro, permite verificação parcial e fornece localização granular de erros.

## Sumário

1.  [Introdução](#1-introdução)
2.  [Terminologia](#2-terminologia)
3.  [Especificação](#3-especificação)
4.  [Implementação](#4-implementação)
5.  [Considerações de Segurança](#5-considerações-de-segurança)
6.  [Exemplos](#6-exemplos)
7.  [Referências](#7-referências)

## 1. Introdução

### 1.1 Motivação

A integridade no nível do arquivo (BSP v1) detecta corrupção, mas fornece informações de diagnóstico limitadas:
*   Não consegue identificar qual chunk está corrompido
*   Requer reconstrução completa do arquivo antes da verificação
*   Desperdiça largura de banda ao baixar novamente o arquivo inteiro em caso de falha

Checksums por chunk resolvem esses problemas ao:
*   Detectar corrupção imediatamente após a recuperação do chunk
*   Localizar erros em chunks específicos
*   Permitir verificação parcial e download seletivo
*   Suportar recursos futuros como verificação em fluxo

### 1.2 Escopo

Esta RFC define:
*   O algoritmo de hashing por chunk (SHA-256)
*   Quando os hashes dos chunks são calculados (durante o chunking)
*   Como os hashes dos chunks são armazenados (metadados compactos)
*   Quando os hashes dos chunks são verificados (durante a reconstrução)
*   Estratégias de tratamento de erros (modos estrito vs. flexível)

### 1.3 Relação com o BSP v1

O BSP v2 é **complementar** ao BSP v1:
*   **SHA-256 por chunk** (BSP v2): Detecta corrupção precocemente, identifica chunks ruins
*   **SHA-256 no nível do arquivo** (BSP v1): Verificação final da reconstrução completa

Ambas as verificações DEVEM passar para a verificação em modo estrito.

## 2. Terminologia

**Termos:**
*   **Chunk**: Um segmento contíguo de dados de arquivo comprimidos
*   **Hash do chunk**: Digest SHA-256 dos bytes do chunk
*   **Índice do chunk**: Posição baseada em zero do chunk no arquivo
*   **Tamanho do chunk**: Número de bytes no chunk (antes da codificação)
*   **Entrada de chunk de metadados**: Array compacto que armazena informações do chunk

## 3. Especificação

### 3.1 Processo de Chunking

Os arquivos são processados da seguinte forma:

```
Arquivo Original
    ↓
[Comprimir com Zstandard nível 19]
    ↓
Dados Comprimidos
    ↓
[Dividir em chunks de tamanho máximo ~1850 bytes]
    ↓
Chunk 0, Chunk 1, ..., Chunk N-1
```

### 3.2 Tamanho do Chunk

#### 3.2.1 Tamanho Máximo

O tamanho máximo do chunk DEVE ser calculado para garantir que o chunk codificado se encaixe nos limites da página da Biblioteca de Babel:

```python
MAX_BABEL_PAGE_SIZE = 3000  # Limite conservador
ENCODING_OVERHEAD = 1.647   # Overhead Base-29

MAX_CHUNK_BYTES = int(MAX_BABEL_PAGE_SIZE / ENCODING_OVERHEAD) - 8
# Resultado: aproximadamente 1813 bytes
```

#### 3.2.2 Tamanho Variável

O último chunk PODE ser menor que `MAX_CHUNK_BYTES` se dados insuficientes permanecerem.

### 3.3 Algoritmo de Hash do Chunk

#### 3.3.1 Seleção do Algoritmo

O algoritmo de hashing do chunk DEVE ser SHA-256 (FIPS 180-4).

**Justificativa:**
*   Criptograficamente seguro (ao contrário do CRC32)
*   Consistente com o hash no nível do arquivo (BSP v1)
*   Permite futuras implementações de árvores Merkle
*   Resistência a colisões de 128 bits suficiente para chunks

#### 3.3.2 Cálculo do Hash

```python
import hashlib

def compute_chunk_hash(chunk_data: bytes) -> str:
    """
    Calcula o hash SHA-256 do chunk.
    
    Args:
        chunk_data: Bytes brutos do chunk (comprimidos)
        
    Returns:
        String hexadecimal em minúsculas (64 caracteres)
    """
    return hashlib.sha256(chunk_data).hexdigest()
```

### 3.4 Formato de Armazenamento

#### 3.4.1 Entrada de Chunk Compacta

Cada chunk é armazenado como um array compacto nos metadados:

**Formato (com coordenadas):**
```json
[tamanho_chunk, hash_chunk, id_hex, parede, prateleira, volume, pagina]
```

**Formato (sem coordenadas):**
```json
[tamanho_chunk, hash_chunk]
```

**Especificações dos campos:**
| Índice | Campo | Tipo | Descrição |
|-------|-------|------|-------------|
| 0 | `tamanho_chunk` | inteiro | Tamanho do chunk em bytes |
| 1 | `hash_chunk` | string | Hash SHA-256 (hex, 64 caracteres) |
| 2 | `id_hex` | string | ID do hexágono (3200 caracteres) |
| 3 | `parede` | inteiro | Número da parede (1-4) |
| 4 | `prateleira` | inteiro | Número da prateleira (1-5) |
| 5 | `volume` | inteiro | Número do volume (1-32) |
| 6 | `pagina` | inteiro | Número da página |

#### 3.4.2 Exemplo Completo de Metadados

```json
{
  "f": "document.pdf",
  "s": 92079,
  "h": "8a3f4f2288a5fba5f76d3c8f5e9a1b4c2d7e3f8g9h0i1j2k3l4m5n6o7p8q9r0",
  "c": 3,
  "v": "v5",
  "chk": [
    [1813, "eff69362b3f87fdb12345678...", "1l8x6v...", 3, 5, 27, 144],
    [1805, "2f4a8d9c1e3b5a7f98765432...", "2k9y7w...", 1, 2, 15, 89],
    [456,  "a1b2c3d4e5f6789012345678...", "3m0z8x...", 2, 4, 8, 256]
  ]
}
```

### 3.5 Verificação

#### 3.5.1 Quando Verificar

Os hashes dos chunks DEVEM ser verificados imediatamente após a decodificação de cada chunk:

```
Chunk Recuperado → [Decodificar do alfabeto de Babel] → Bytes Decodificados
                                                       ↓
                                              [Truncar para tamanho_chunk]
                                                       ↓
                                               [Calcular SHA-256]
                                                       ↓
                                            [Comparar com metadados]
```

#### 3.5.2 Algoritmo de Verificação

```python
def verify_chunk(chunk_data: bytes, expected_hash: str, 
                 strict: bool = False) -> bool:
    """
    Verifica a integridade do chunk.
    
    Args:
        chunk_data: Bytes brutos do chunk
        expected_hash: SHA-256 esperado dos metadados
        strict: Se True, levanta exceção em caso de incompatibilidade
        
    Returns:
        True se os hashes corresponderem
        
    Raises:
        RuntimeError: Se strict e os hashes não corresponderem
    """
    actual_hash = hashlib.sha256(chunk_data).hexdigest()
    
    if actual_hash != expected_hash.lower():
        if strict:
            raise RuntimeError(
                f"Incompatibilidade de SHA-256 do chunk:\n"
                f"  Esperado: {expected_hash}\n"
                f"  Obtido:      {actual_hash}"
            )
        else:
            print(f"AVISO: Incompatibilidade de hash do chunk", file=sys.stderr)
            return False
    
    return True
```

#### 3.5.3 Modos de Falha

**Modo Estrito (`--strict`):**
*   Primeira incompatibilidade de hash → Abortar imediatamente
*   Excluir reconstrução parcial
*   Sair com status de erro
*   Relatar qual chunk falhou

**Modo Normal (padrão):**
*   Registrar aviso no stderr
*   Continuar com os chunks restantes
*   Tentar reconstrução
*   A verificação final no nível do arquivo detectará a corrupção

### 3.6 Ordenação de Chunks

#### 3.6.1 Ordem Canônica

Os chunks DEVEM ser armazenados e processados em ordem crescente pelo `chunk_index`:

```
Chunk 0 → Chunk 1 → Chunk 2 → ... → Chunk N-1
```

#### 3.6.2 Reconstrução

Durante a reconstrução, os chunks DEVEM ser concatenados em ordem canônica:

```python
# Ordenar chunks por índice
sorted_chunks = sorted(chunks_data, key=lambda x: x[0])

# Concatenar em ordem
compressed_data = bytearray()
for chunk_index, chunk_bytes in sorted_chunks:
    compressed_data.extend(chunk_bytes)
```

## 4. Implementação

### 4.1 Implementação de Chunking

Veja `file_chunker.py`:

```python
def split_file_into_chunks(filepath: str, 
                          chunk_size: int = MAX_CHUNK_BYTES):
    """
    Comprime o arquivo e o divide em chunks.
    
    Yields:
        Tuplas (chunk_index, chunk_data)
    """
    # Ler e comprimir arquivo
    with open(filepath, \'rb\') as f:
        data = f.read()
    
    cctx = zstd.ZstdCompressor(level=19)
    compressed_data = cctx.compress(data)
    
    # Gerar chunks
    for chunk_index, offset in enumerate(
        range(0, len(compressed_data), chunk_size)
    ):
        chunk = compressed_data[offset:offset + chunk_size]
        yield chunk_index, chunk
```

### 4.2 Criação de Metadados

```python
def create_file_metadata(filepath: str) -> FileMetadata:
    """Cria metadados com hashes por chunk."""
    # ... hash no nível do arquivo (BSP v1) ...
    
    chunks = []
    for chunk_index, chunk_data in split_file_into_chunks(filepath):
        # BSP v2: Calcular hash por chunk
        chunk_hash = hashlib.sha256(chunk_data).hexdigest()
        
        chunks.append(ChunkMetadata(
            chunk_index=chunk_index,
            chunk_size=len(chunk_data),
            chunk_hash=chunk_hash,
            babel_coords={}  # Preenchido durante o upload
        ))
    
    return FileMetadata(
        # ... outros campos ...
        chunks=chunks
    )
```

### 4.3 Verificação Durante o Download

```python
def download_and_verify_chunk(chunk_metadata: ChunkMetadata, 
                              strict: bool = False) -> bytes:
    """
    Baixa o chunk e verifica a integridade.
    
    Returns:
        Bytes do chunk verificado
        
    Raises:
        RuntimeError: Se strict e a verificação falhar
    """
    coords = chunk_metadata.babel_coords
    
    # Recuperar da Babel
    encoded = babel.browse(
        coords["hex"], coords["wall"], 
        coords["shelf"], coords["volume"], 
        coords["page"]
    )
    
    # Decodificar
    chunk_data = binary_encoder.decode_babel_to_bytes(encoded)
    chunk_data = chunk_data[:chunk_metadata.chunk_size]
    
    # BSP v2: Verificar hash do chunk
    actual_hash = hashlib.sha256(chunk_data).hexdigest()
    expected_hash = chunk_metadata.chunk_hash
    
    if actual_hash != expected_hash:
        if strict:
            raise RuntimeError(
                f"Chunk {chunk_metadata.chunk_index} falhou na verificação"
            )
        else:
            print(f"AVISO: Chunk {chunk_metadata.chunk_index} "
                  f"incompatibilidade de hash", file=sys.stderr)
    
    return chunk_data
```

### 5.1 Resistência a Colisões

SHA-256 fornece resistência a colisões de 128 bits por chunk. Com tamanhos de chunk típicos (~1850 bytes), a probabilidade de colisões naturais é desprezível.

### 5.2 Vetores de Ataque

**Protegido Contra:**
*   Corrupção de chunk único (detectada imediatamente)
*   Corrupção de múltiplos chunks (cada um detectado independentemente)
*   Reconstrução de chunk fora de ordem (validado por índice)
*   Downloads incompletos (incompatibilidade na contagem de chunks)

**Não Protegido Contra:**
*   Adulteração de metadados (requer assinaturas BSP v4)
*   Substituição de chunk válido inteiro (requer árvore Merkle)

### 5.3 Impacto no Desempenho

**Overhead de Armazenamento:**
*   64 bytes por chunk (string hexadecimal)
*   Para arquivo de 100MB (~2200 chunks): ~140KB de metadados
*   Desprezível em comparação com o armazenamento de coordenadas

**Overhead de Computação:**
*   SHA-256: ~400-600 MB/s em CPUs modernas
*   Para chunk de 1850 bytes: ~0.003ms
*   Desprezível em comparação com a latência da rede

### 5.4 Comparação com Alternativas

| Algoritmo | Tamanho da Saída | Resistência a Colisões | Velocidade | Criptográfico |
|-----------|------------------|------------------------|------------|---------------|
| CRC32 | 4 bytes | 32-bit | Muito rápido |  Não |
| MD5 | 16 bytes | Quebrado | Rápido |  Não |
| SHA-1 | 20 bytes | Quebrado | Rápido |  Não |
| **SHA-256** | **32 bytes** | **128-bit** | **Rápido** | **Sim** |
| SHA-512 | 64 bytes | 256-bit | Rápido |  Sim |
| BLAKE3 | 32 bytes | 128-bit | Muito rápido |  Sim |

SHA-256 escolhido pelo equilíbrio entre segurança, velocidade e amplo suporte.

## 6. Exemplos

### 6.1 Fluxo Completo de Upload

```python
# 1. Criar metadados com hashes de chunk
metadata = create_file_metadata("document.pdf")

# Exemplo de saída:
# metadata.chunks = [
#   ChunkMetadata(
#     chunk_index=0,
#     chunk_size=1813,
#     chunk_hash="eff69362b3f87fdb...",
#     babel_coords={}
#   ),
#   ChunkMetadata(
#     chunk_index=1,
#     chunk_size=1805,
#     chunk_hash="2f4a8d9c1e3b5a7f...",
#     babel_coords={}
#   ),
#   # ... mais chunks ...
# ]

# 2. Fazer upload de cada chunk
for chunk_meta in metadata.chunks:
    # Obter dados do chunk
    chunk_data = get_chunk_data(chunk_meta.chunk_index)
    
    # Verificar hash antes do upload (verificação de sanidade)
    assert hashlib.sha256(chunk_data).hexdigest() == chunk_meta.chunk_hash
    
    # Codificar e buscar
    encoded = binary_encoder.encode_bytes_to_babel(chunk_data)
    coords = babel.search(encoded)
    
    # Armazenar coordenadas
    chunk_meta.babel_coords = coords

# 3. Salvar metadados
metadata.save("document.json.gz")
```

### 6.2 Fluxo Completo de Download com Verificação

```python
# 1. Carregar metadados
metadata = FileMetadata.load("document.json.gz")

# 2. Baixar e verificar cada chunk
verified_chunks = []

for chunk_meta in metadata.chunks:
    print(f"Baixando chunk {chunk_meta.chunk_index + 1}/"
          f"{len(metadata.chunks)}...")
    
    # Baixar
    coords = chunk_meta.babel_coords
    encoded = babel.browse(coords["hex"], coords["wall"], 
                          coords["shelf"], coords["volume"], 
                          coords["page"])
    
    # Decodificar
    chunk_data = binary_encoder.decode_babel_to_bytes(encoded)
    chunk_data = chunk_data[:chunk_meta.chunk_size]
    
    # BSP v2: Verificar hash do chunk
    actual_hash = hashlib.sha256(chunk_data).hexdigest()
    
    if actual_hash != chunk_meta.chunk_hash:
        raise RuntimeError(f"Chunk {chunk_meta.chunk_index} falhou na "
                          f"verificação SHA-256!")
    
    print(f"  ✓ SHA-256 do chunk verificado")
    verified_chunks.append((chunk_meta.chunk_index, chunk_data))

# 3. Reconstruir arquivo
reconstruct_file_from_chunks(verified_chunks, metadata, 
                             "restored.pdf", strict=True)

# 4. BSP v1: Verificação final no nível do arquivo
final_hash = calculate_file_hash("restored.pdf")
if final_hash != metadata.file_hash:
    raise RuntimeError("A verificação SHA-256 no nível do arquivo falhou!")

print("✓ Download completo e verificado!")
```

### 6.3 Vetores de Teste

**Chunk de Entrada**: `b"hello world"` (11 bytes)
```
SHA-256: b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9
```

**Chunk de Entrada**: 1850 bytes de zeros
```
SHA-256: 474747474747474747474747474747474747474747474747474747474747...
```

## 7. Referências

### 7.1 Referências Normativas

*   [FIPS 180-4](https://csrc.nist.gov/publications/detail/fips/180/4/final) - Padrão de Hash Seguro
*   [RFC 0001](rfc-0001.md) - Integridade no Nível do Arquivo (BSP v1)

### 7.2 Referências Informativas

*   [RFC 0003](rfc-0003.md) - Especificação de Codificação Binária
*   [RFC 0005](rfc-0005.md) - Modo Estrito (BSP v5)
*   [RFC 0006](rfc-0006.md) - Extensões Futuras (Árvores Merkle)

## Apêndice A: Migração do CRC32

Se estiver atualizando de uma implementação legada de CRC32:

1.  Atualize `file_chunker.py` para calcular SHA-256 em vez de CRC32
2.  Atualize o formato de metadados para armazenar strings hexadecimais em vez de inteiros
3.  Mantenha a compatibilidade retroativa aceitando ambos os formatos
4.  Faça o re-upload dos arquivos para gerar metadados SHA-256

## Apêndice B: Registro de Alterações

*   **2026-02-16**: Versão inicial (especificação BSP v2 com SHA-256)

---

**Aviso de Direitos Autorais**: Este documento é lançado sob a Licença MIT.
